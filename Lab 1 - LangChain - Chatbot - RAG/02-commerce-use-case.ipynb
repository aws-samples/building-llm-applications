{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "aa60fae7-36b0-47d4-8c8f-8ce3b400b5e2",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Chatbot use case : Flower commerce"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c522cdd4-86b9-4b2d-9f1e-0ec659e8c4e5",
   "metadata": {},
   "source": [
    "Let's try to apply the knowledge we have so far on prompt engineering and Langchain to create a prototype of a chatbot. We will use a use case of flower commerce where customers can order flowers. This will include ability to extract intention and information from the prompt to be used in SQL query."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b3bb476c-9467-4f1d-8513-83e0718453e5",
   "metadata": {},
   "source": [
    "**NOTE**: This notebook provides examples on handling information extraction from prompts and data handling in database, which are presented in a way to make education easier, but **may not represent best practices nor production grade code**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "22fbc923-6cd5-450a-a39d-daa8c32d29f9",
   "metadata": {},
   "source": [
    "## 1. Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0edd5b3f-ee77-4deb-a5a9-e23be694dc81",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install --upgrade -q pip\n",
    "!pip install --upgrade -q langchain\n",
    "!pip install -q transformers\n",
    "!pip install -q faiss-gpu\n",
    "!pip install -q bs4\n",
    "!pip install -q sentence-transformers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39a5018e-45c2-46be-bba2-476f8522171c",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import regex\n",
    "import json\n",
    "import time\n",
    "import sagemaker, boto3\n",
    "import re\n",
    "from sagemaker.session import Session\n",
    "from sagemaker.model import Model\n",
    "from sagemaker import image_uris, model_uris, script_uris, hyperparameters\n",
    "from sagemaker.predictor import Predictor\n",
    "from sagemaker.utils import name_from_base\n",
    "from typing import Any, Dict, List, Optional\n",
    "from langchain.embeddings import SagemakerEndpointEmbeddings\n",
    "from langchain.llms.sagemaker_endpoint import ContentHandlerBase\n",
    "\n",
    "# secure the Sagemaker session, role, region, and other details to work with Sagemaker APIs\n",
    "sagemaker_session = Session()\n",
    "aws_role = sagemaker_session.get_caller_identity_arn()\n",
    "aws_region = boto3.Session().region_name\n",
    "sm_client = boto3.client(\"sagemaker\", aws_region)\n",
    "sess = sagemaker.Session()\n",
    "model_version = \"*\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "65e18636-fc1e-42e2-bc9b-c7481f23d432",
   "metadata": {},
   "source": [
    "Define method to invoke the SageMaker endpoint who hosts the LLM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f245145-8b57-4d4c-998a-f3d8296628cf",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# functions that work with parsing JSON messages going to and from our Sagemaker Endpoint \n",
    "# (i.e. Deployed Sagemaker Jumpstart LLM) \n",
    "\n",
    "# invoke the Sagemaker Endpoint with the user query\n",
    "def query_endpoint_with_json_payload(encoded_json, endpoint_name, content_type=\"application/json\"):\n",
    "    client = boto3.client(\"runtime.sagemaker\")\n",
    "    response = client.invoke_endpoint(\n",
    "        EndpointName=endpoint_name, ContentType=content_type, Body=encoded_json\n",
    "    )\n",
    "    return response\n",
    "\n",
    "# parse the Sagemaker Endpoint response to the user query\n",
    "def parse_response_model(query_response):\n",
    "    model_predictions = json.loads(query_response[\"Body\"].read())\n",
    "    return [gen[\"generated_text\"] for gen in model_predictions]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "43ae54f8-9195-4bd1-8dd8-112c172a7268",
   "metadata": {},
   "source": [
    "Configure the SageMaker Real-time Endpoint to use"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a925544-4f25-4eb3-8ef7-2b71ed9c38b3",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "_MODEL_CONFIG_ = {\n",
    "     \"falcon-7b\" : {\n",
    "        \"aws_region\": \"us-west-2\",\n",
    "        \"endpoint_name\": \"jumpstart-dft-hf-llm-falcon-7b-instruct-bf16\",\n",
    "        \"parse_function\": parse_response_model\n",
    "    },\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a71018c3-8ad7-4a14-9ba6-0ec63a8924ca",
   "metadata": {},
   "source": [
    "Configure LangChain integration with SageMaker Endpoint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea78db67-f0dd-4377-b02a-2cdcb36b7478",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from langchain.prompts.prompt import PromptTemplate\n",
    "from langchain.llms.sagemaker_endpoint import LLMContentHandler, SagemakerEndpoint\n",
    "\n",
    "parameters ={\n",
    "        \"max_new_tokens\": 100,\n",
    "        \"num_return_sequences\": 1,\n",
    "        \"top_k\": 1,\n",
    "        \"do_sample\": False,\n",
    "        \"return_full_text\": False,\n",
    "        \"temperature\": 0.1,\n",
    "        \"stop\": [\"\\n\\n\"]\n",
    "}\n",
    "\n",
    "class ContentHandler(LLMContentHandler):\n",
    "    content_type = \"application/json\"\n",
    "    accepts = \"application/json\"\n",
    "\n",
    "    def transform_input(self, prompt: str, model_kwargs={}) -> bytes:\n",
    "        input_str = json.dumps({\"inputs\": prompt, \"parameters\": model_kwargs})\n",
    "        return input_str.encode(\"utf-8\")\n",
    "\n",
    "    def transform_output(self, output: bytes) -> str:\n",
    "        response_json = json.loads(output.read().decode(\"utf-8\"))\n",
    "        return response_json[0][\"generated_text\"]\n",
    "\n",
    "\n",
    "content_handler = ContentHandler()\n",
    "\n",
    "# Passing the SagemakerEndPoint to LangChain so that it knows where to send all the inference requests\n",
    "sm_llm_falcon_instruct = SagemakerEndpoint(\n",
    "    endpoint_name=_MODEL_CONFIG_[\"falcon-7b\"][\"endpoint_name\"],\n",
    "    region_name=_MODEL_CONFIG_[\"falcon-7b\"][\"aws_region\"],\n",
    "    model_kwargs=parameters,\n",
    "    content_handler=content_handler,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "20f3ccd3-748f-4af0-bd1d-ceae046f4409",
   "metadata": {},
   "source": [
    "## 2. Configure database"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a163c121-00d5-48b1-9e8b-9a191fd4c37f",
   "metadata": {
    "tags": []
   },
   "source": [
    "Sometimes, after we capture the intention of our customer in our chatbot, we may need a connection to a database to serve the query. In this lab, we will build a simple shop assistant prototype which can connect to database to serve the intention captured by the LLM from the customer's dialog."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5c43b1d2-250f-4129-baa5-977553e9ae1b",
   "metadata": {},
   "source": [
    "Now, let's configure the database. Let's pretend we are a flower shop which sells only 3 types of flowers for now: French Rose, Sunflower, and Dahlia. The database uses a local SQLite. It has 'flowers' and 'orders' table. We also define the method to create order."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bec03a19-e224-48a8-a9c7-5d6228259884",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import sqlite3\n",
    "from sqlite3 import Error\n",
    "\n",
    "class DB():\n",
    "    def __init__(self, file_name):\n",
    "        self.file_name = file_name\n",
    "        self.conn = None\n",
    "\n",
    "    def create_connection(self):\n",
    "        try:\n",
    "            self.conn = sqlite3.connect(self.file_name)\n",
    "        except Error as e:\n",
    "            print(e)\n",
    "\n",
    "        return self.conn\n",
    "            \n",
    "    def create_table(self, create_table_sql):\n",
    "        if self.conn == None:\n",
    "            self.conn = self.create_connection()\n",
    "        try:\n",
    "            c = self.conn.cursor()\n",
    "            c.execute(create_table_sql)\n",
    "        except Error as e:\n",
    "            print(e)\n",
    "        self.close_conn()\n",
    "\n",
    "    def create_flowers_table(self):\n",
    "        sql_create_flowers_table = \"\"\" CREATE TABLE IF NOT EXISTS flowers (\n",
    "                                        id integer PRIMARY KEY AUTOINCREMENT,\n",
    "                                        name text NOT NULL,\n",
    "                                        unit_price text,\n",
    "                                        stocks integer CHECK(stocks >= 0)\n",
    "                                    ); \"\"\"\n",
    "        self.create_table(sql_create_flowers_table)\n",
    "\n",
    "    def create_orders_table(self):\n",
    "        sql_create_orders_table = \"\"\" CREATE TABLE IF NOT EXISTS orders (\n",
    "                                        id integer PRIMARY KEY AUTOINCREMENT,\n",
    "                                        flower_id integer NOT NULL,\n",
    "                                        units integer,\n",
    "                                        total_price real,\n",
    "                                        time text NOT NULL,\n",
    "                                        poem text,\n",
    "                                        FOREIGN KEY (flower_id) REFERENCES flowers (id)\n",
    "                                    ); \"\"\"\n",
    "        self.create_table(sql_create_orders_table)\n",
    "\n",
    "    def insert_flowers(self):\n",
    "        if self.conn == None:\n",
    "            self.conn = self.create_connection()\n",
    "            \n",
    "        sql = ''' INSERT INTO flowers(name,unit_price,stocks)\n",
    "                  VALUES ('French Rose',5.4,2), ('Sunflower',7.2,3), ('Dahlia',3.0,30), ('Lily',10.5,13)'''\n",
    "        cur = self.conn.cursor()\n",
    "        cur.execute(sql)\n",
    "        self.conn.commit()\n",
    "        last_row_id = cur.lastrowid\n",
    "        self.close_conn()\n",
    "        return last_row_id\n",
    "\n",
    "    def create_order(self, flower: str, quantity: int) -> str:\n",
    "        if self.conn == None:\n",
    "            self.conn = self.create_connection()\n",
    "\n",
    "        # Check first if the flower exists and whether there is stock\n",
    "        sql = f\"SELECT * FROM flowers WHERE name LIKE '{flower}';\"\n",
    "        print(f\"\"\"\n",
    "            Running this SQL statement first:\n",
    "            {sql}\n",
    "        \"\"\")\n",
    "\n",
    "        cur = self.conn.cursor()\n",
    "        cur.execute(sql)\n",
    "        row = cur.fetchone()\n",
    "        print(f\"\"\"\n",
    "            Results:\n",
    "            id, name, unit_price, stocks\n",
    "            {row}\n",
    "        \"\"\")\n",
    "\n",
    "        if row is None:\n",
    "            return f\"I am sorry that we do not sell {flower}. Please order Dahlia, Lily, French Rose, or Sunflower.\"\n",
    "        else:\n",
    "            flower_name = row[1]\n",
    "            if (row[3]  - quantity) < 0:\n",
    "                return f\"I am sorry, there is no stock available for {quantity} of {flower_name}\"\n",
    "            else:\n",
    "                total_price = float(quantity) * float(row[2])\n",
    "                cur = self.conn.cursor()\n",
    "\n",
    "                sql1 = f\"\"\"INSERT INTO orders(flower_id,units,total_price,time)\n",
    "                          VALUES({row[0]},{quantity},{total_price},{int(time.time())});\"\"\"\n",
    "                sql2 = f\"\"\"UPDATE flowers SET stocks = stocks - {quantity} WHERE id = {row[0]};\"\"\"\n",
    "                print(f\"\"\"\n",
    "                    Then running these SQL statements in a transaction:\n",
    "                    {sql1}\n",
    "                    {sql2}\n",
    "                \"\"\")\n",
    "\n",
    "                cur.execute(sql1)\n",
    "                cur.execute(sql2)\n",
    "\n",
    "                try:\n",
    "                    self.conn.commit()\n",
    "                    return f\"The shop has {quantity} {flower_name} in stock. The order for {quantity} {flower_name} has been completed with order_flower tool\"\n",
    "                except Error as e:\n",
    "                    print(e)\n",
    "                    return \"failed\"\n",
    "        self.close_conn()\n",
    "\n",
    "    def view_orders(self):\n",
    "        sql = f\"SELECT * FROM orders;\"\n",
    "        print(f\"Running this SQL statement: {sql}\")\n",
    "        self.conn = self.create_connection()\n",
    "        cur = self.conn.cursor()\n",
    "        rows = cur.execute(sql)\n",
    "        print(f\"Results:\")\n",
    "        print(\"id, flower_id, quantity, total_price, time, poem\")\n",
    "        for row in rows:\n",
    "            print(row)\n",
    "        self.close_conn()\n",
    "    \n",
    "    def add_poem(self, order_id, poem):\n",
    "        if self.conn == None:\n",
    "            self.conn = self.create_connection(self.file_name)\n",
    "\n",
    "        # Check first if the flower exists and whether there is stock\n",
    "        sql = f\"UPDATE orders SET poem='{poem}' WHERE id={order_id};\"\n",
    "        cur = self.conn.cursor()\n",
    "        cur.execute(sql)\n",
    "        self.conn.commit()\n",
    "        self.close_conn()\n",
    "        return f\"The poem is created and stored.\"\n",
    "\n",
    "    def close_conn(self):\n",
    "        self.conn.close()\n",
    "        self.conn = None\n",
    "            \n",
    "db = DB(\"flowers.db\")\n",
    "db.create_flowers_table()\n",
    "db.create_orders_table()\n",
    "db.insert_flowers()\n",
    "time.sleep(15)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b68ad0a-a151-41ef-8346-6f7c1165c811",
   "metadata": {},
   "source": [
    "## 3. Prompt engineering (few shots) for extracting both intention and order information"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cfa04475-06dd-4f07-9c31-4140c75be9d9",
   "metadata": {},
   "source": [
    "Let's first validate that the database does not have any order yet\n",
    "\n",
    "Note: If you see any orders already, just delete the **flowers.db** file in this same directory and run the notebook again from top."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a46c29a-3817-4fbb-986b-d2e3779048f4",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "db.view_orders()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "814dc3e1-befe-479c-906f-82ef90d4a8bb",
   "metadata": {},
   "source": [
    "Use few-shots technique to teach the LLM to extract the flower name and quantity requested from the conversation\n",
    "\n",
    "We want the LLM to take the prompt and give us the extracted flower name and quantity being ordered in the form of (after post-processing):\n",
    "\n",
    "```\n",
    "{\"intention\": \"order_flower\", \"flower\": \"Jasmine\", \"quantity\": \"1\"}\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "49a6cd83-9661-439e-bdb4-fed0156a30c1",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from langchain import PromptTemplate\n",
    "from langchain.prompts.few_shot import FewShotPromptTemplate\n",
    "\n",
    "prefix = \"\"\"You are an intelligent flower shop assistant. Customer can order flowers. Extract the information on the flower name and quantity being ordered. \n",
    "Look at the examples below.\"\"\"\n",
    "\n",
    "examples = [\n",
    "    {\n",
    "        \"history\":\"\",\n",
    "        \"customer\": \"I buy 2 French roses. Please send tomorrow.\",\n",
    "        \"extracted_information\":  \"{{\\\"intention\\\": \\\"order_flower\\\", \\\"flower\\\":\\\"French Rose\\\", \\\"quantity\\\":\\\"2\\\"}}\"\n",
    "    },\n",
    "    {\n",
    "        \"history\": \"Customer Example: Let me order sunflowers.\" + \\\n",
    "                   \"Shop AI Assistant: Sure. How many Sunflowers do you want to order?\",\n",
    "        \"customer\": \"Four please.\",\n",
    "        \"extracted_information\":  \"{{\\\"intention\\\": \\\"order_flower\\\", \\\"flower\\\":\\\"Sunflower\\\", \\\"quantity\\\":\\\"4\\\"}}\"\n",
    "    },\n",
    "]\n",
    "\n",
    "template = \"\"\"{history}\n",
    "Customer Example: {customer}\n",
    "Extracted Information: {extracted_information}\n",
    "\"\"\"\n",
    "example_prompt = PromptTemplate(input_variables=[\"history\",\"customer\",\"extracted_information\"], template=template)\n",
    "\n",
    "suffix = \"\"\"Given the examples above, extract the information from the customer's request / question below.\n",
    "\n",
    "{history}\n",
    "Customer: {input}\n",
    "Extracted Information:\"\"\"\n",
    "\n",
    "prompt = FewShotPromptTemplate(\n",
    "    examples=examples, \n",
    "    example_prompt=example_prompt, \n",
    "    prefix=prefix,\n",
    "    suffix=suffix, \n",
    "    input_variables=[\"history\", \"input\"]\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "701778ec-3ae5-4875-927a-a2243ae1e703",
   "metadata": {},
   "source": [
    "Let's define the method to ask the chatbot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e72372c-878f-4635-967c-dd28f7b16155",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def ask_chatbot(history, input):\n",
    "    entry = prompt.format(history=history,input=input)\n",
    "    # Print the prompt\n",
    "    print(entry)\n",
    "    return sm_llm_falcon_instruct(entry)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a419a9c-4505-4927-b034-08d2ccc503da",
   "metadata": {},
   "source": [
    "Now let's ask the chatbot, which will call the Falcon-7b-instruct model in SageMaker"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d0ca0d7a-58f4-4d3f-ac86-3db90f26d313",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "history = \"\"\n",
    "input = \"Hello, can I get 2 Dahlia please?\"\n",
    "\n",
    "response = ask_chatbot(history, input)\n",
    "response"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0e3ffd7b-ca88-415b-bbe4-891df4784d56",
   "metadata": {},
   "source": [
    "Let's try one where the information is spread in the history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "92b6a775-82c1-4b33-9a46-8044cfeca441",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "history = \"\"\"\n",
    "Customer: Please send me Dahlia flowers tomorrow.\n",
    "Shop AI Assistant: Sure. How many?\"\"\"\n",
    "\n",
    "input = \"Two please.\"\n",
    "\n",
    "response = ask_chatbot(history, input)\n",
    "response"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8818a52b-8dde-4d20-8fac-094dc650c6fb",
   "metadata": {},
   "source": [
    "Now, let's extract the information from the JSON into a Python's dictionary so that information can be passed on to the SQL query more easily."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b3ea5b67-52e0-4b08-a21b-777c4b893ab4",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def extract_json_from_answer(answer):\n",
    "    answer = answer.replace(\"\\n\",\"\\\\n\")\n",
    "    pattern = regex.compile(r'\\{(?:[^{}]|(?R))*\\}')\n",
    "    task_json = json.loads(pattern.search(answer).group(0))\n",
    "    return task_json\n",
    "task = extract_json_from_answer(response)\n",
    "task"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7f0976e0-4dc1-4dd4-bca2-6c81832d6602",
   "metadata": {
    "tags": []
   },
   "source": [
    "## 4. Make database change based on extracted information"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "035d9bcc-cf5b-4707-9ec8-a9ae0df33fc6",
   "metadata": {
    "tags": []
   },
   "source": [
    "Now that we know what to be ordered, let's make the SQL query to simulate a real purchase"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e02609f-9373-4829-b13d-dd5ed35d88b2",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "db.create_order(task['flower'], int(task['quantity']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f239232-5c5a-43ac-bd51-34872c9583ab",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "db.view_orders()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a6d669d-8c8a-44f4-bedb-9141ebaa8d9a",
   "metadata": {
    "tags": []
   },
   "source": [
    "## 5. Integrating with LangChain's tools and agent"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "09468399-01e4-4515-9fc2-35f948c0843b",
   "metadata": {},
   "source": [
    "So far we have been using prompts only. Now we want to use LangChain's agent to try automating the intent and information extraction and driving the completion until we have an answer for the customer."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f319ff3e-83c9-499d-8483-4fec2e04690a",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Defining agent's prompt"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9691fc6c-0080-47c9-9aa2-8af899dd7dbb",
   "metadata": {},
   "source": [
    "Let's modify the previous few-shots prompt template to adopt the ReAct framework for the agent."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c77d1c7-dba3-433c-88d5-227b2aba408d",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from langchain import PromptTemplate\n",
    "from langchain.prompts.few_shot import FewShotPromptTemplate\n",
    "\n",
    "SYSTEM_MESSAGE = \"\"\"You are an intelligent flower shop assistant who assists Customer to order flower or do other things.\n",
    "Flower order must have both flower name and flower quantity. If one or more information is missing, ask the customer back.\n",
    "When there is no stock, do not continue the order. When we do not sell the flower, do not continue the order.\"\"\"\n",
    "\n",
    "few_shot_template_prefix = \"\"\"{system_message}\n",
    "\n",
    "You have access to these tools:\n",
    "{tools_information}\n",
    "\n",
    "Use the following format:\n",
    "\n",
    "Customer: the input question you must answer\n",
    "Thought: you should always think about what to do\n",
    "Action: the action to take, should be one of [{tool_names}]\n",
    "Action Input: the input to the action\n",
    "Observation: the result of the action\n",
    "... (this Thought/Action/Action Input/Observation can repeat N times)\n",
    "Thought: I now know the final answer\n",
    "Shop AI Assistant: the final answer to the original input question\n",
    "\n",
    "See the examples below:\"\"\"\n",
    "\n",
    "few_shots = [\n",
    "    {\n",
    "        \"history\":\"\",\n",
    "        \"customer\": \"I buy 2 French roses. Please send tomorrow.\",\n",
    "        \"agent_scratchpad\":  \"Thought: The Customer wants to order flower[French Rose] with quantity[2]. All information is complete. I can proceed with creating order with order_flower tool.\\n\" + \\\n",
    "                             \"Action: order_flower\\n\" + \\\n",
    "                             \"Action Inputs: French Rose, 2\\n\" + \\\n",
    "                             \"Observation: The shop has 2 French Rose in stock. The order for 2 French Rose has been completed with order_flower tool.\\n\" + \\\n",
    "                             \"Thought: Now I know how to respond to the Customer\\n\" +\\\n",
    "                             \"Shop AI Assistant: Your order for 2 French Rose has been successfully completed. Thank you for shopping with us.\\n\"\n",
    "    },\n",
    "    {\n",
    "        \"history\": \"Customer: Let me order sunflowers.\\n\" + \\\n",
    "                   \"Shop AI Assistant: Sure. How many Sunflowers do you want to order?\",\n",
    "        \"customer\": \"Four please.\",\n",
    "        \"agent_scratchpad\":  \"Thought: The Customer wants to order flower[Sunflower] with quantity[4]. All information is complete. I can proceed with creating order with order_flower tool.\\n\" + \\\n",
    "                             \"Action: order_flower\\n\" + \\\n",
    "                             \"Action Inputs: Sunflower, 4\\n\" + \\\n",
    "                             \"Observation: The shop has 4 Sunflower in stock. The order for 4 Sunflower has been completed with order_flower tool.\\n\" + \\\n",
    "                             \"Thought: Now I know how to respond to the Customer\\n\" +\\\n",
    "                             \"Shop AI Assistant: Your order for 4 Sunflower has been successfully completed. Have a nice day!\\n\"\n",
    "    },\n",
    "    {\n",
    "        \"history\": \"\",\n",
    "        \"customer\": \"Can I get 10 jasmines?\",\n",
    "        \"agent_scratchpad\":  \"Thought: The Customer wants to order flower[Jasmine] with quantity[10]. All information is complete. I can proceed with creating order with order_flower tool.\\n\" + \\\n",
    "                             \"Action: order_flower\\n\" + \\\n",
    "                             \"Action Inputs: Jasmine, 10\\n\" + \\\n",
    "                             \"Observation: I am sorry, there is no stock available for 10 of Jasmine.\\n\" + \\\n",
    "                             \"Thought: Now I know how to respond to the customer\\n\" +\\\n",
    "                             \"Shop AI Assistant: I am sorry, we do not have enough stock for 10 of Jasmine flowers.\\n\"\n",
    "    }\n",
    "]\n",
    "\n",
    "each_shot_template = \"\"\"{history}\n",
    "Customer: {customer}\n",
    "{agent_scratchpad}\n",
    "\"\"\"\n",
    "\n",
    "each_shot_prompt_template = PromptTemplate(input_variables=[\"history\",\"customer\",\"agent_scratchpad\"], template=each_shot_template)\n",
    "\n",
    "few_shot_template_suffix = \"\"\"Given the examples above, assist the Customer based on the recent conversation below.\n",
    "\n",
    "{history}\n",
    "Customer: {input}\n",
    "{agent_scratchpad}\"\"\"\n",
    "\n",
    "agent_template = FewShotPromptTemplate(\n",
    "    examples=few_shots, \n",
    "    example_prompt=each_shot_prompt_template, \n",
    "    prefix=few_shot_template_prefix,\n",
    "    suffix=few_shot_template_suffix, \n",
    "    input_variables=[\"history\",\"agent_scratchpad\",\"input\",\"tools_information\", \"tool_names\", \"system_message\"]\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d830e9b-58c5-4e42-8e45-555522f8e658",
   "metadata": {},
   "source": [
    "Redefine a simple method to invoke the LLM using the LangChain prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d6c652c7-47e9-40b5-83e0-255b45181c95",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "tools_information = \"\"\"\n",
    "order_flower: Useful for completing the flowers order.\n",
    "\"\"\"\n",
    "tool_names = \"order_flower\"\n",
    "STOP_TOKENS = [\"\\nObservation:\"]\n",
    "sm_llm_falcon_instruct.model_kwargs[\"stop\"] = STOP_TOKENS\n",
    "def ask_chatbot(history, agent_scratchpad, input):\n",
    "    entry = agent_template.format(history=history,agent_scratchpad=agent_scratchpad,input=input, tools_information=tools_information, tool_names=tool_names, system_message=SYSTEM_MESSAGE)\n",
    "    # Print the prompt\n",
    "    print(entry)\n",
    "    return sm_llm_falcon_instruct(entry)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f922f0a-5287-4d92-8894-0917990bc45c",
   "metadata": {},
   "source": [
    "Test how the LLM now behaves with the new prompt template. First we test with a scenario where customer's information is complete and order is to be made using order_flower tool."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "781da7d7-6748-4378-adba-9ea5efb13b54",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "history = \"\"\n",
    "agent_scratchpad =  \"\"\n",
    "input = \"Can I get 2 Dahlia flowers please?\"\n",
    "\n",
    "response = ask_chatbot(history, agent_scratchpad, input)\n",
    "response"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a3b8092-0fc8-47d6-be68-171622a1b5e2",
   "metadata": {},
   "source": [
    "Now assume the agent has managed to call the order_flower tool successfully and the ordered is inserted into the database. It should now update the customer."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3fc9b33d-808e-445b-a1c1-2cef1b3413d7",
   "metadata": {},
   "source": [
    "### Creating tools"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9cc0bb85-cffd-42ff-a9b2-52ded6fa4dcf",
   "metadata": {},
   "source": [
    "Let's now create the LangChain tools for fulfilling the intentions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c8692b40-fd81-441e-9475-0b16990256bd",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from langchain.tools import BaseTool, StructuredTool, Tool, tool\n",
    "\n",
    "def order_flower(string):\n",
    "    \"\"\"\"useful for when you need to make an order to buy lily flowers. \n",
    "    The input to this tool should be a comma separated list string and number of length two, representing the flower and quantity you want to buy. \n",
    "    For example, `Lily, 3` would be the input if you wanted to make an order for Lily flowers of quantity 3.\"\"\"\n",
    "    flower, quantity = string.split(\",\")\n",
    "    return db.create_order(flower.strip(), int(quantity.strip()))\n",
    "\n",
    "tools = [\n",
    "    Tool(\n",
    "        name=\"order_flower\",\n",
    "        func=order_flower,\n",
    "        description=\"Useful for completing the flowers order\"\n",
    "    )\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3f86b9b9-dbb1-47b7-acc9-32de60926226",
   "metadata": {},
   "source": [
    "### Creating Langchain Agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e044ffa-e246-4aae-8ed7-747c66359163",
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Union\n",
    "from langchain.agents import AgentExecutor, LLMSingleActionAgent, AgentOutputParser\n",
    "from langchain.schema import AgentAction, AgentFinish\n",
    "\n",
    "class CustomOutputParser(AgentOutputParser):\n",
    "    \n",
    "    def parse(self, llm_output: str) -> Union[AgentAction, AgentFinish]:\n",
    "        print(f'llm_output={llm_output}')\n",
    "            \n",
    "        if \"Shop AI Assistant\" in llm_output:\n",
    "            try: \n",
    "                lines = llm_output.split(\"\\n\")\n",
    "                for line in lines:\n",
    "                    if \"Shop AI Assistant\" in line:\n",
    "                        llm_output = line\n",
    "                        break\n",
    "            except:\n",
    "                pass\n",
    "            return AgentFinish(\n",
    "                return_values={\"output\": llm_output.split(\"Shop AI Assistant:\")[-1].strip()},\n",
    "                log=llm_output,\n",
    "            )\n",
    "        \n",
    "        if \"Action Inputs\" in llm_output:\n",
    "            regex = r\"Action\\s*\\d*\\s*:(.*?)[\\n]Action\\s*\\d*\\s*Inputs\\s*\\d*\\s*:[\\s]*(.*)\"\n",
    "            match = re.search(regex, llm_output, re.DOTALL)\n",
    "            if not match:\n",
    "                raise ValueError(f\"Could not parse LLM output: `{llm_output}`\")\n",
    "\n",
    "            action = match.group(1).strip()\n",
    "            action_input = match.group(2).strip()\n",
    "            return AgentAction(tool=action, tool_input=action_input.strip(\" \").strip('\"'), log=llm_output)\n",
    "\n",
    "        raise ValueError(f\"Could not parse LLM output: `{llm_output}`\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5d71dc69-aaf1-4be2-ac5c-89a492298a83",
   "metadata": {},
   "source": [
    "Next, we will also define the prompt template to handle the \"intermediate_steps\" variable defined in the agent."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30edb939-74be-4356-8bce-6d203d0027cb",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from langchain.prompts import StringPromptTemplate\n",
    "\n",
    "class CustomPromptTemplate(StringPromptTemplate):\n",
    "    template: FewShotPromptTemplate\n",
    "    tools: List[Tool]\n",
    "    \n",
    "    def format(self, **kwargs) -> str:\n",
    "        intermediate_steps = kwargs.pop(\"intermediate_steps\")\n",
    "        print(f\"intermediate_steps:{intermediate_steps}\")\n",
    "        thoughts = \"\"\n",
    "        for action, observation in intermediate_steps:\n",
    "            thoughts += action.log\n",
    "            #thoughts += f\"\\nObservation: {observation}\\nThought: \"\n",
    "            thoughts += f\"\\nObservation: {observation}\"\n",
    "        \n",
    "        kwargs[\"agent_scratchpad\"] = thoughts\n",
    "        \n",
    "        kwargs[\"tools_information\"] = \"\\n\".join([f\"{tool.name}: {tool.description}\" for tool in self.tools])\n",
    "        kwargs[\"tool_names\"] = \", \".join([tool.name for tool in self.tools])\n",
    "        kwargs[\"system_message\"] = SYSTEM_MESSAGE\n",
    "\n",
    "        return self.template.format(**kwargs)\n",
    "    \n",
    "agent_prompt = CustomPromptTemplate(\n",
    "    template=agent_template,\n",
    "    tools=tools,\n",
    "    input_variables=[\"input\", \"intermediate_steps\", \"history\"]\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eef21e00-1c88-4336-a61a-8c03baa8fa95",
   "metadata": {},
   "source": [
    "Next we define the chain and the agent. We use memory to allow the prompt to extract information from the chat history."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a1b0812f-4624-4b05-886e-1f602859a9e9",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from langchain import LLMChain\n",
    "from langchain.memory import ConversationBufferMemory, ConversationSummaryBufferMemory\n",
    "\n",
    "\n",
    "llm_chain = LLMChain(llm=sm_llm_falcon_instruct, prompt=agent_prompt)\n",
    "\n",
    "agent = LLMSingleActionAgent(\n",
    "    llm_chain=llm_chain, \n",
    "    output_parser=CustomOutputParser(),\n",
    "    stop=STOP_TOKENS, \n",
    "    allowed_tools=[tool.name for tool in tools]\n",
    ")\n",
    "\n",
    "\n",
    "memory = ConversationBufferMemory(\n",
    "    memory_key=\"history\",\n",
    "    human_prefix=\"Customer\",\n",
    "    ai_prefix=\"Shop AI Assistant\"\n",
    ")\n",
    "\n",
    "agent_chain = AgentExecutor.from_agent_and_tools(agent=agent, tools=tools, memory=memory, verbose=True)\n",
    "agent_chain.agent.llm_chain.verbose=True\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2790c163-71de-4daf-856d-b31b2a7d1f49",
   "metadata": {},
   "source": [
    "Let's try the agent. With the agent, now the call to the LLM, the information extraction, and the fulfillment of the intent (e.g. inserting data to DB) can be done in one step from the user perspective."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0437bc91-4070-44b4-9f62-38de1f5a50a6",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "agent_chain.run(input=\"Can I get 11 pieces of lily for next week?\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1667766a-23e5-4ebc-b26d-b5a14bdac347",
   "metadata": {},
   "source": [
    "Let's verify by looking at the 'orders' table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d84d8e3-ac36-4bad-9708-50aca926047a",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "db.view_orders()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e425bf37-3a53-4511-8d97-d26dc77c7c74",
   "metadata": {},
   "source": [
    "Now let's try to order again when it is already out of stock"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "606d78cb-825c-4ea3-9329-64a9baa75f19",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "agent_chain.run(input=\"Can I get 11 lily for next week?\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "70e0f0b1-51ca-47ec-9ac7-52f29cc042e3",
   "metadata": {},
   "source": [
    "Let's now challenge it with ability to collect more information when not present."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce1257c4-7d43-4fad-910c-3dda009788dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "agent_chain.run(input=\"Let me order blue orchids\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae3ee149-2cab-4c64-94f6-46b5345ab144",
   "metadata": {},
   "source": [
    "Let's try a more advanced scenario where the flower name and quantity are not in 1 message. Rather they are spread in the conversation."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2611331d-abb6-48eb-bd73-7620b6ece032",
   "metadata": {
    "tags": []
   },
   "source": [
    "## 5. Add 'create poem' intent by leveraging LLM's generative capability"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4678ff48-289a-4521-8418-7c6d7bc04cac",
   "metadata": {},
   "source": [
    "As the title implies, let's add the third intent to create poem. This also demonstrates the LLM's creative generative capability\n",
    "\n",
    "First, we add a new example in the prompt's few shots, to reflect the intention to create poem and add it to a flower order."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b4bfddfd-e40c-4bda-8941-29f0cfca33d3",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "few_shots.append({\n",
    "    \"history\":\"\",\n",
    "    \"customer\": \"Please add a poem of how much I love my mother in her mother's day. My order_id is 1.\",\n",
    "    \"agent_scratchpad\":  \"Thought: The Customer requested a poem for order_id[1] and provided the topic. I can continue with create_poem tool.\\n\" + \\\n",
    "                         \"Action: create_poem\\n\" + \\\n",
    "                         \"Action Inputs: 1, how much I love my mother in her mother's day.\\n\" + \\\n",
    "                         \"Observation: The poem is created and stored.\\n\" + \\\n",
    "                         \"Thought: Now I know how to respond to the Customer\\n\" +\\\n",
    "                         \"Shop AI Assistant: Your poem has been added to the order. Check your order to see the poem.\\n\"\n",
    "})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e1cc0fe-8a81-4cec-a78f-a866d5ba0fce",
   "metadata": {},
   "source": [
    "Then we define the query to the database, the function to fulfil the intent, and reinitialize the agent chain."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0369ff5d-b120-4bec-87ef-b7dd32682ad4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_poem(string):\n",
    "    poem_args = string.split(\",\")\n",
    "    order_id = poem_args[0]\n",
    "    topic = \",\".join(poem_args[1:])\n",
    "    prompt = f\"\"\"\n",
    "    Write a poem about how much I love my mother in her mother's day.   \n",
    "    Poem: Upon your gentle care, I softly lay\\\\nUpon your heart, I would peacefully starate\\\\nTo think of what you've done, I'd count each day\\\\nIn gratefulness, I'd thank you, my mother, for being there.\n",
    "    \n",
    "    Write a poem about {topic}\n",
    "    Poem: \n",
    "    \"\"\"\n",
    "    initial_args = sm_llm_falcon_instruct.model_kwargs.copy()\n",
    "    sm_llm_falcon_instruct.model_kwargs[\"temperature\"] = 0.9\n",
    "    sm_llm_falcon_instruct.model_kwargs[\"max_new_token\"] = 50\n",
    "    sm_llm_falcon_instruct.model_kwargs[\"stop\"] = [\"\\n\\n\\n\"]\n",
    "    \n",
    "    poem = sm_llm_falcon_instruct(prompt)\n",
    "    \n",
    "    sm_llm_falcon_instruct.model_kwargs = initial_args.copy()\n",
    "    \n",
    "    poem = poem.replace(\"'\",\"''\")\n",
    "    print(f\"\"\"Poem generated:\n",
    "        {poem}\n",
    "    \"\"\")\n",
    "    print(\"Adding poem to database\")\n",
    "    return db.add_poem(order_id, poem)\n",
    "\n",
    "tools.append(Tool(\n",
    "    name=\"create_poem\",\n",
    "    func=create_poem,\n",
    "    description=\"Useful for creating poem\"\n",
    "))\n",
    "\n",
    "agent_template = FewShotPromptTemplate(\n",
    "    examples=few_shots, \n",
    "    example_prompt=each_shot_prompt_template, \n",
    "    prefix=few_shot_template_prefix,\n",
    "    suffix=few_shot_template_suffix, \n",
    "    input_variables=[\"history\",\"agent_scratchpad\",\"input\",\"tools_information\", \"tool_names\", \"system_message\"]\n",
    ")\n",
    "\n",
    "agent_prompt = CustomPromptTemplate(\n",
    "    template=agent_template,\n",
    "    tools=tools,\n",
    "    input_variables=[\"input\", \"intermediate_steps\", \"history\"]\n",
    ")\n",
    "\n",
    "llm_chain = LLMChain(llm=sm_llm_falcon_instruct, prompt=agent_prompt)\n",
    "\n",
    "agent = LLMSingleActionAgent(\n",
    "    llm_chain=llm_chain, \n",
    "    output_parser=CustomOutputParser(),\n",
    "    stop=STOP_TOKENS, \n",
    "    allowed_tools=[tool.name for tool in tools]\n",
    ")\n",
    "\n",
    "\n",
    "memory = ConversationBufferMemory(\n",
    "    memory_key=\"history\",\n",
    "    human_prefix=\"Customer\",\n",
    "    ai_prefix=\"Shop AI Assistant\"\n",
    ")\n",
    "\n",
    "agent_chain = AgentExecutor.from_agent_and_tools(agent=agent, tools=tools, memory=memory, verbose=True)\n",
    "agent_chain.agent.llm_chain.verbose=True"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "29fa3247-8581-44ef-b9f7-24051e862cdc",
   "metadata": {},
   "source": [
    "Let's test it out."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "49219e29-a77c-4bb9-bde1-22d74ca6d45b",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "agent_chain.run(input=\"Please add a poem of how I am thankful to my teacher, Rendy. My order_id is 1.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "caa403a0-a3c1-4880-872b-73adb38ff2fc",
   "metadata": {},
   "source": [
    "Of course we will define the prompt template and the few-shots example."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f926626-ccea-49c7-aec5-640671a1fa63",
   "metadata": {
    "tags": []
   },
   "source": [
    "Now let's ask the LLM again to create the poem then store it in the database"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "971a0d11-acd4-426a-94a2-226dbdef4d93",
   "metadata": {},
   "source": [
    "Let's check the orders table to see if the poem is already added."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a963587-4194-4f73-8ab1-cf50198f7827",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "db.view_orders()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "27a5fddf-01a4-4024-8a54-412c7900ac66",
   "metadata": {
    "tags": []
   },
   "source": [
    "## 6. Add RAG"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ceb35d6-f171-4864-98bf-b2b8498c71c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from langchain.embeddings import HuggingFaceEmbeddings\n",
    "from langchain.vectorstores import FAISS\n",
    "from langchain.document_loaders import TextLoader\n",
    "\n",
    "# Define the txt2embedding model\n",
    "sm_llm_embeddings = HuggingFaceEmbeddings()\n",
    "\n",
    "# Load documents\n",
    "loader = TextLoader(file_path=\"./Flower_Shop_FAQs.txt\")\n",
    "documents = loader.load()\n",
    "\n",
    "# Get your splitter ready\n",
    "text_splitter = RecursiveCharacterTextSplitter(chunk_size=200, chunk_overlap=0)\n",
    "\n",
    "# Split your docs into texts\n",
    "texts = text_splitter.split_documents(documents)\n",
    "\n",
    "# Embedd your texts\n",
    "docsearch = FAISS.from_documents(texts, sm_llm_embeddings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c00c9da-6bbf-429c-933b-ee0c044e2e15",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "few_shots.append({\n",
    "    \"history\":\"\",\n",
    "    \"customer\": \"Do you do international delivery?\",\n",
    "    \"agent_scratchpad\":  \"Thought: The Customer asked a question about shop FAQs. I can continue with shop_faqs tool.\\n\" + \\\n",
    "                         \"Action: shop_faqs\\n\" + \\\n",
    "                         \"Action Inputs: Do you do international delivery?\\n\" + \\\n",
    "                         \"Observation: No, only local delivery is supported\\n\" + \\\n",
    "                         \"Thought: Now I know how to respond to the Customer\\n\" +\\\n",
    "                         \"Shop AI Assistant: I am sorry, we only do local delivery for now.\"\n",
    "})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "81cfaf5d-c02d-422a-af70-5f84870597ee",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def ask_faqs(question):\n",
    "    docs = docsearch.similarity_search(question, k=1)\n",
    "    docs_string = \"\\n\\n\".join(list(map(lambda x: x.page_content, docs)))\n",
    "    print(\"\\nDocuments found:\")\n",
    "    print(docs_string)\n",
    "    prompt = f\"\"\"\n",
    "    ===Context===\n",
    "    {docs_string}\n",
    "    ===End of Context===\n",
    "    Given the context above, answer the question below. ONLY use information from the context. Do not make up any new information.\n",
    "    Question: {question}\n",
    "    Answer:\"\"\"\n",
    "    print(\"\\nRAG prompt:\")\n",
    "    print(prompt)\n",
    "    initial_args = sm_llm_falcon_instruct.model_kwargs.copy()\n",
    "    sm_llm_falcon_instruct.model_kwargs[\"max_new_token\"] = 50\n",
    "    sm_llm_falcon_instruct.model_kwargs[\"stop\"] = [\"\\n\"]\n",
    "\n",
    "    answer = sm_llm_falcon_instruct(prompt)\n",
    "    \n",
    "    sm_llm_falcon_instruct.model_kwargs = initial_args.copy()\n",
    "    \n",
    "    return answer\n",
    "\n",
    "tools.append(Tool(\n",
    "    name=\"shop_faqs\",\n",
    "    func=ask_faqs,\n",
    "    description=\"Useful for answering questions about this purchase methods, return policy, delivery, and related matters.\"\n",
    "))\n",
    "\n",
    "agent_template = FewShotPromptTemplate(\n",
    "    examples=few_shots, \n",
    "    example_prompt=each_shot_prompt_template, \n",
    "    prefix=few_shot_template_prefix,\n",
    "    suffix=few_shot_template_suffix, \n",
    "    input_variables=[\"history\",\"agent_scratchpad\",\"input\",\"tools_information\", \"tool_names\", \"system_message\"]\n",
    ")\n",
    "\n",
    "agent_prompt = CustomPromptTemplate(\n",
    "    template=agent_template,\n",
    "    tools=tools,\n",
    "    input_variables=[\"input\", \"intermediate_steps\", \"history\"]\n",
    ")\n",
    "\n",
    "llm_chain = LLMChain(llm=sm_llm_falcon_instruct, prompt=agent_prompt)\n",
    "\n",
    "agent = LLMSingleActionAgent(\n",
    "    llm_chain=llm_chain, \n",
    "    output_parser=CustomOutputParser(),\n",
    "    stop=STOP_TOKENS, \n",
    "    allowed_tools=[tool.name for tool in tools]\n",
    ")\n",
    "\n",
    "\n",
    "memory = ConversationBufferMemory(\n",
    "    memory_key=\"history\",\n",
    "    human_prefix=\"Customer\",\n",
    "    ai_prefix=\"Shop AI Assistant\"\n",
    ")\n",
    "\n",
    "agent_chain = AgentExecutor.from_agent_and_tools(agent=agent, tools=tools, memory=memory, verbose=True)\n",
    "agent_chain.agent.llm_chain.verbose=True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "caa8c634-b777-4f3a-85c6-dfb15ce7372f",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "agent_chain.run(input=\"Do you provide international delivery?\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fb5bc37f-293f-4f32-9b99-a5ca475d7ea9",
   "metadata": {
    "tags": []
   },
   "source": [
    "## 7. UX"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "68c3f635-1da0-42f2-8d63-bd50867662ef",
   "metadata": {},
   "source": [
    "Let's have a simple UX"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "213ae1b3-f473-4c5b-95c5-8ce6bc965912",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import ipywidgets as ipw\n",
    "from IPython.display import display, clear_output\n",
    "\n",
    "from io import StringIO\n",
    "import sys\n",
    "import textwrap\n",
    "\n",
    "def print_ww(*args, width: int = 100, **kwargs):\n",
    "    \"\"\"Like print(), but wraps output to `width` characters (default 100)\"\"\"\n",
    "    buffer = StringIO()\n",
    "    try:\n",
    "        _stdout = sys.stdout\n",
    "        sys.stdout = buffer\n",
    "        print(*args, **kwargs)\n",
    "        output = buffer.getvalue()\n",
    "    finally:\n",
    "        sys.stdout = _stdout\n",
    "    for line in output.splitlines():\n",
    "        print(\"\\n\".join(textwrap.wrap(line, width=width)))\n",
    "\n",
    "class ChatUX:\n",
    "    \"\"\" A chat UX using IPWidgets\n",
    "    \"\"\"\n",
    "    def __init__(self, qa, retrievalChain = False):\n",
    "        self.qa = qa\n",
    "        self.name = None\n",
    "        self.b=None\n",
    "        self.retrievalChain = retrievalChain\n",
    "        self.out = ipw.Output()\n",
    "\n",
    "\n",
    "    def start_chat(self):\n",
    "        print(\"Starting chat bot\")\n",
    "        display(self.out)\n",
    "        self.chat(None)\n",
    "\n",
    "\n",
    "    def chat(self, _):\n",
    "        if self.name is None:\n",
    "            prompt = \"\"\n",
    "        else: \n",
    "            prompt = self.name.value\n",
    "        if 'q' == prompt or 'quit' == prompt or 'Q' == prompt:\n",
    "            print(\"Thank you , that was a nice chat !!\")\n",
    "            return\n",
    "        elif len(prompt) > 0:\n",
    "            with self.out:\n",
    "                thinking = ipw.Label(value=\"Thinking...\")\n",
    "                display(thinking)\n",
    "                try:\n",
    "                    if self.retrievalChain:\n",
    "                        result = self.qa.run({'question': prompt })\n",
    "                    else:\n",
    "                        result = self.qa.run({'input': prompt }) #, 'history':chat_history})\n",
    "                except:\n",
    "                    result = \"No answer\"\n",
    "                thinking.value=\"\"\n",
    "                print_ww(f\"Shop AI Assistant:{result}\")\n",
    "                self.name.disabled = True\n",
    "                self.b.disabled = True\n",
    "                self.name = None\n",
    "\n",
    "        if self.name is None:\n",
    "            with self.out:\n",
    "                self.name = ipw.Text(description=\"Customer:\", placeholder='q to quit')\n",
    "                self.b = ipw.Button(description=\"Send\")\n",
    "                self.b.on_click(self.chat)\n",
    "                display(ipw.Box(children=(self.name, self.b)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "59037e91-beb7-4ce5-8f6a-8d7d4a4ae5a2",
   "metadata": {},
   "source": [
    "Now feel free to play around with the chat input. Remember, it uses memory, so it may load your earlier chat as well.\n",
    "\n",
    "You will notice that sometimes the result is not what we expected. We can probably improve this by adding more few-shots, or fine-tuning the model, or just better prompt. Feel free to explore!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "06133c0c-93f6-4ce5-b156-66d74a369151",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "chat = ChatUX(agent_chain)\n",
    "chat.start_chat()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd4d7fc0-f28b-42fc-8424-8106781c485e",
   "metadata": {},
   "source": [
    "## 8. Clean up"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fbed0434-f3b2-4584-9bbe-f76c64213767",
   "metadata": {},
   "source": [
    "Please refer to the **Lab 1 - Cleanup section of the workshop's instruction**. Especially, you need to delete the SageMaker endpoint you spun up for hosting the Falcon-7b-instruct model we used in the workshop so far"
   ]
  }
 ],
 "metadata": {
  "availableInstances": [
   {
    "_defaultOrder": 0,
    "_isFastLaunch": true,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 4,
    "name": "ml.t3.medium",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 1,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 8,
    "name": "ml.t3.large",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 2,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.t3.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 3,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.t3.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 4,
    "_isFastLaunch": true,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 8,
    "name": "ml.m5.large",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 5,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.m5.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 6,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.m5.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 7,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 64,
    "name": "ml.m5.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 8,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 128,
    "name": "ml.m5.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 9,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 192,
    "name": "ml.m5.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 10,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 256,
    "name": "ml.m5.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 11,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 384,
    "name": "ml.m5.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 12,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 8,
    "name": "ml.m5d.large",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 13,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.m5d.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 14,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.m5d.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 15,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 64,
    "name": "ml.m5d.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 16,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 128,
    "name": "ml.m5d.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 17,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 192,
    "name": "ml.m5d.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 18,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 256,
    "name": "ml.m5d.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 19,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 384,
    "name": "ml.m5d.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 20,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": true,
    "memoryGiB": 0,
    "name": "ml.geospatial.interactive",
    "supportedImageNames": [
     "sagemaker-geospatial-v1-0"
    ],
    "vcpuNum": 0
   },
   {
    "_defaultOrder": 21,
    "_isFastLaunch": true,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 4,
    "name": "ml.c5.large",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 22,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 8,
    "name": "ml.c5.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 23,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.c5.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 24,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.c5.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 25,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 72,
    "name": "ml.c5.9xlarge",
    "vcpuNum": 36
   },
   {
    "_defaultOrder": 26,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 96,
    "name": "ml.c5.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 27,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 144,
    "name": "ml.c5.18xlarge",
    "vcpuNum": 72
   },
   {
    "_defaultOrder": 28,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 192,
    "name": "ml.c5.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 29,
    "_isFastLaunch": true,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.g4dn.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 30,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.g4dn.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 31,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 64,
    "name": "ml.g4dn.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 32,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 128,
    "name": "ml.g4dn.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 33,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 4,
    "hideHardwareSpecs": false,
    "memoryGiB": 192,
    "name": "ml.g4dn.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 34,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 256,
    "name": "ml.g4dn.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 35,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 61,
    "name": "ml.p3.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 36,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 4,
    "hideHardwareSpecs": false,
    "memoryGiB": 244,
    "name": "ml.p3.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 37,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 8,
    "hideHardwareSpecs": false,
    "memoryGiB": 488,
    "name": "ml.p3.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 38,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 8,
    "hideHardwareSpecs": false,
    "memoryGiB": 768,
    "name": "ml.p3dn.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 39,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.r5.large",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 40,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.r5.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 41,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 64,
    "name": "ml.r5.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 42,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 128,
    "name": "ml.r5.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 43,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 256,
    "name": "ml.r5.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 44,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 384,
    "name": "ml.r5.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 45,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 512,
    "name": "ml.r5.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 46,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 768,
    "name": "ml.r5.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 47,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.g5.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 48,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.g5.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 49,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 64,
    "name": "ml.g5.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 50,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 128,
    "name": "ml.g5.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 51,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 256,
    "name": "ml.g5.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 52,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 4,
    "hideHardwareSpecs": false,
    "memoryGiB": 192,
    "name": "ml.g5.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 53,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 4,
    "hideHardwareSpecs": false,
    "memoryGiB": 384,
    "name": "ml.g5.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 54,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 8,
    "hideHardwareSpecs": false,
    "memoryGiB": 768,
    "name": "ml.g5.48xlarge",
    "vcpuNum": 192
   },
   {
    "_defaultOrder": 55,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 8,
    "hideHardwareSpecs": false,
    "memoryGiB": 1152,
    "name": "ml.p4d.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 56,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 8,
    "hideHardwareSpecs": false,
    "memoryGiB": 1152,
    "name": "ml.p4de.24xlarge",
    "vcpuNum": 96
   }
  ],
  "instance_type": "ml.m5.large",
  "kernelspec": {
   "display_name": "Python 3 (Data Science 3.0)",
   "language": "python",
   "name": "python3__SAGEMAKER_INTERNAL__arn:aws:sagemaker:us-west-2:236514542706:image/sagemaker-data-science-310-v1"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
